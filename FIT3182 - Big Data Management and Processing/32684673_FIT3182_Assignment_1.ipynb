{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oD5VW0TzHdR_"
   },
   "source": [
    "# FIT3182 - Assignment 1 - Questions Workbook\n",
    "\n",
    "This assignment consists of **four questions** that are, taken together, worth 10% of the final marks. **Please treat this Assignment as a take-home test. This is an individual assignment and you should complete the questions on your own**. You should not attempt to post questions on EdForum seeking solutions to the answers. If you require clarification on the Assignment questions, you can post a private post on EdForum or seek consultation from the tutors.\n",
    "\n",
    "1. The first question is related to **Parallel Search Algorithms (2 Mark)**.\n",
    "2. The second question is related to **Parallel Join Algorithms (4 Marks)**.\n",
    "3. The third question is related to **Parallel Sort Algorithms (2 Marks)**.\n",
    "4. The fourth question is related to **Parallel GroupBy Algorithms (2 Marks)**.\n",
    "\n",
    "**Instructions:**\n",
    "- You will be using Python 3. Answer all questions inside this Jupyter Notebook. Please use the provided Docker to load the Jupyter Notebook.\n",
    "- Read the instructions, skeleton code, and comments carefully.\n",
    "- There are **code blocks that you need to complete** yourself as a part of the assignment. \n",
    "- You are also required to **answer the questions below**. \n",
    "- **Comment each line of code properly such that the tutor can easily understand what you are trying to do in the code. Marks may be deducted for insufficient or unclear comments.**\n",
    "- Once completed, please rename the Jupyter Notebook to include your Student ID at the beginning of the filename (e.g., 12345678_FIT3182_Assignment_1.ipynb). Submit this Jupyter Notebook into the Assignment 1 submission link in Moodle. Please refer to the Assignment 1 submission link (or page) in Moodle for information about the submission due date.\n",
    "\n",
    "**Please include your details as follows:**\n",
    "- Student ID: \n",
    "- Last Name: \n",
    "- First (or Preferred) Name:\n",
    "- Monash Student Email: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRWxd1YrHdSB"
   },
   "source": [
    "## Dataset\n",
    "For this test, we will use the following tables R and S to write solutions to three parallel algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImhBOqgmHdSC"
   },
   "outputs": [],
   "source": [
    "# R consists of 15 pairs, each comprising two attributes (nominal and numeric)\n",
    "R = [('Adele',8),('Bob',22),('Clement',16),('Dave',23),('Ed',11),\n",
    "     ('Fung',25),('Goel',3),('Harry',17),('Irene',14),('Joanna',2),\n",
    "     ('Kelly',6),('Lim',20),('Meng',1),('Noor',5),('Omar',19)]\n",
    "\n",
    "# S consists of 8 pairs, each comprising two attributes (nominal and numeric)\n",
    "S = [('Arts',8),('Business',15),('CompSc',2),('Dance',12),('Engineering',7),\n",
    "     ('Finance',21),('Geology',10),('Health',11),('IT',18)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7W3uUvJLErBY",
    "tags": []
   },
   "source": [
    "## 1. Parallel Searching Algorithm (2 marks)\n",
    "**This section consist of 4 sub-questions.**\n",
    "\n",
    "In this task, you will build a **parallel search algorithm for range selection (continuous)** for a given query.\n",
    "\n",
    " **Implement a parallel search algorithm** that uses local linear search (i.e., `linear_search()`) and is able to work with the hash partitioning method (i.e., `h_partition()`).\n",
    " **Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Local Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CvTYiOkyFJp8"
   },
   "outputs": [],
   "source": [
    "# Linear search function\n",
    "def linear_search(data, key):\n",
    "    \"\"\"\n",
    "    Perform linear search on data for the given key.\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset (list OR numpy array)\n",
    "    key -- an query attribute (number)\n",
    "\n",
    "    Return:\n",
    "    result --  list of matched records such as [('Adele', 8)]\n",
    "                or the position of searched record\n",
    "                whichever is more appropriate for your code\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in data:\n",
    "        if i[1] == key:\n",
    "            matches.append(i)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 755,
     "status": "ok",
     "timestamp": 1555295423504,
     "user": {
      "displayName": "Prajwol Sangat",
      "photoUrl": "https://lh4.googleusercontent.com/-jjsf-xy1nFE/AAAAAAAAAAI/AAAAAAAAAmM/hDKToPDDstc/s64/photo.jpg",
      "userId": "16045204576665706371"
     },
     "user_tz": -600
    },
    "id": "ruGmDOnfmPWc",
    "outputId": "494f8520-91c4-470d-e640-0bc5a6e184f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Lim', 20)]\n"
     ]
    }
   ],
   "source": [
    "results = linear_search(R, 20)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Hash Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "An_0xFW2FQvs"
   },
   "outputs": [],
   "source": [
    "# Define a simple hash function.\n",
    "def s_hash(x, n):\n",
    "    \"\"\"\n",
    "    Define a simple hash function for demonstration\n",
    "\n",
    "    Arguments:\n",
    "    x -- an input record attribute (int)\n",
    "    n -- the number of processors (int)\n",
    "\n",
    "    Return:\n",
    "    result -- the hash value of x\n",
    "    \"\"\"\n",
    "    return x % n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMLZCK2BFYeF"
   },
   "outputs": [],
   "source": [
    "# Hash data partitioning function. \n",
    "# We will use the s_hash function defined above\n",
    "def h_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform hash data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset (list)\n",
    "    n -- the number of processors (int)\n",
    "\n",
    "    Return:\n",
    "    result -- the partitioned subsets of D\n",
    "    \"\"\"\n",
    "    partitions = {p:[] for p in range(n)}\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for x in data:\n",
    "        h=s_hash(x[1],n)\n",
    "        partitions[h].append(x)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 873,
     "status": "ok",
     "timestamp": 1555295448743,
     "user": {
      "displayName": "Prajwol Sangat",
      "photoUrl": "https://lh4.googleusercontent.com/-jjsf-xy1nFE/AAAAAAAAAAI/AAAAAAAAAmM/hDKToPDDstc/s64/photo.jpg",
      "userId": "16045204576665706371"
     },
     "user_tz": -600
    },
    "id": "Ako86IBFpqwj",
    "outputId": "dd8f1dc8-a73f-4455-f059-58cc0e79a01f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [('Fung', 25), ('Lim', 20), ('Noor', 5)],\n",
      " 1: [('Clement', 16), ('Ed', 11), ('Kelly', 6), ('Meng', 1)],\n",
      " 2: [('Bob', 22), ('Harry', 17), ('Joanna', 2)],\n",
      " 3: [('Adele', 8), ('Dave', 23), ('Goel', 3)],\n",
      " 4: [('Irene', 14), ('Omar', 19)]}\n"
     ]
    }
   ],
   "source": [
    "partitions = h_partition(R, 5)\n",
    "pprint(partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parallel Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlVKTCO-FkV9"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def collect_result(results, result):\n",
    "    # Callback for when linear_search returns a result.\n",
    "    # The results list is modified only by the main process, not the pool workers.\n",
    "    results.append(result)\n",
    "\n",
    "# Parallel searching algorithm for range selection\n",
    "def parallel_search_range(data, query_range, n_processor):\n",
    "    \"\"\"\n",
    "    Perform parallel search for range selection on data for the given key.\n",
    "\n",
    "    Arguments:\n",
    "    data -- the input dataset (list)\n",
    "    query_range -- a tuple with inclusive range end-points (e.g. [30, 50] means the interval 30-50)\n",
    "    n_processor -- the number of parallel processors (int)\n",
    "    \n",
    "    Return:\n",
    "    results -- the matched record information\n",
    "    \"\"\"\n",
    "    results=[]\n",
    "    pool = Pool(processes=n_processor)\n",
    "    new_data = h_partition(data,n_processor)\n",
    "        \n",
    "    ### START CODE HERE ###    \n",
    "    callbacks=[]\n",
    "    for i in range (query_range[0],query_range[1]+1):\n",
    "        query_hash = s_hash(i, n_processor)\n",
    "#         print(i,new_data[query_hash])\n",
    "        callbacks.append(pool.apply_async(linear_search,(list(new_data[query_hash]),i)))\n",
    "    for item in callbacks:\n",
    "#         print(item)\n",
    "        for i in item.get():\n",
    "            collect_result(results,i)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1555295758812,
     "user": {
      "displayName": "Prajwol Sangat",
      "photoUrl": "https://lh4.googleusercontent.com/-jjsf-xy1nFE/AAAAAAAAAAI/AAAAAAAAAmM/hDKToPDDstc/s64/photo.jpg",
      "userId": "16045204576665706371"
     },
     "user_tz": -600
    },
    "id": "vgfN1IcyFxaG",
    "outputId": "5fa6eb31-b14b-441e-fdad-a6767369df34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Noor', 5), ('Kelly', 6), ('Adele', 8), ('Ed', 11), ('Irene', 14)]\n"
     ]
    }
   ],
   "source": [
    "n_processor = 3\n",
    "results = parallel_search_range(R, [5, 15], n_processor)\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Parallel Search Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Briefly answer the following.**\n",
    "\n",
    "1. How would the parallel search you implemented need to be changed if we switched to round-robin partition for the data? Provide a small snippet of code with the modified search loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answers**:\n",
    "    By using Round Robin partition we will not be able to find the specific partition to search so we need to loop through 3 of the partition. But for the hash partition we can first hashed the query key to get the specific partition and then we just need to search the value in the partition.\n",
    "    \n",
    "    #Snippet of code#\n",
    "    new_data = RoundRobin_partition(data,n_processor)\n",
    "          \n",
    "    callbacks=[]\n",
    "    for i in range (query_range[0],query_range[1]+1):\n",
    "        for each_partition in new_data:\n",
    "            callbacks.append(pool.apply_async(linear_search,(each_partition,i)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qf29MfmUGWV1"
   },
   "source": [
    "## 2. Parallel Join Algorithm (4 marks)\n",
    "**This section consist of 5 sub-questions.**\n",
    "\n",
    "In this task, you will implement a **disjoint-partitioning based parallel join algorithm**. This algorithm consist of two stages.\n",
    "1. Data disjoint-partitioning across processors.\n",
    "1. Local join in each processor.\n",
    "\n",
    "As a data partitioning method, use the range partitioning algorithm  (i.e. `range_partition( )`). Assume that we have **3 parallel processors**, processor 1 will get records with join attribute value between 1 and 9, processor 2 between 10 and 19, and processor 3 between 20 and 29. Note that both tables R and S need to be partitioned using the same ranges described earlier.\n",
    "\n",
    "As a joining technique, use the hash based join algorithm (i.e., `HB_join( )`).  **Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Range Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_BwmzrmHaTN"
   },
   "outputs": [],
   "source": [
    "# Range data partitionining function (Need to modify as instructed above)\n",
    "def range_partition(data, range_indices):\n",
    "    \"\"\"\n",
    "    Perform range data partitioning on data based on the join attribute\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset (list)\n",
    "    range_indices -- the range end-points (e.g., [5, 10, 15] means\n",
    "                        4 intervals: x < 5, 5 <= x < 10, 10 <= x < 15, 15 <= x)\n",
    "\n",
    "    Return:\n",
    "    result -- the partitioned subsets of D\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    ### START CODE HERE ###  \n",
    "    new_data = list(data)\n",
    "    new_data = sorted(new_data,key=lambda x: x[1])\n",
    "    n_bin = len(range_indices)\n",
    "    \n",
    "    for i in range(n_bin): \n",
    "        s = [x for x in new_data if x[1] < range_indices[i]] \n",
    "        result.append(s) \n",
    "        last_element = s[len(s)-1]\n",
    "        last = new_data.index(last_element)\n",
    "        new_data = new_data[int(last)+1:] \n",
    "\n",
    "        \n",
    "    result.append(new_data) \n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4eA2G_7vafZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Meng', 1),\n",
      "  ('Joanna', 2),\n",
      "  ('Goel', 3),\n",
      "  ('Noor', 5),\n",
      "  ('Kelly', 6),\n",
      "  ('Adele', 8)],\n",
      " [('Ed', 11), ('Irene', 14), ('Clement', 16), ('Harry', 17), ('Omar', 19)],\n",
      " [('Lim', 20), ('Bob', 22), ('Dave', 23), ('Fung', 25)]]\n"
     ]
    }
   ],
   "source": [
    "pprint(range_partition(R, [10, 20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kONrahsIMmD"
   },
   "outputs": [],
   "source": [
    "def H(r):\n",
    "    \"\"\"\n",
    "    We define a hash function 'H' that is used in the hashing process works \n",
    "    by summing the first and second digits of the hashed attribute, which\n",
    "    in this case is the join attribute. \n",
    "    \n",
    "    Arguments:\n",
    "    r -- a record where hashing will be applied on its join attribute\n",
    "\n",
    "    Return:\n",
    "    result -- the hash index of the record r\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the value of the join attribute into the digits\n",
    "    digits = [int(d) for d in str(r[1])]\n",
    "    \n",
    "    # Calulate the sum of elemenets in the digits\n",
    "    return sum(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gEpFbToJIPlr"
   },
   "outputs": [],
   "source": [
    "def HB_join(T1, T2):\n",
    "    \"\"\"\n",
    "    Perform the hash-based join algorithm.\n",
    "    The join attribute is the numeric attribute (in second position) in the input tables T1 & T2.\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    dic = {} # We will use a dictionary\n",
    "    \n",
    "    # For each record in table T2\n",
    "    for s in T2:\n",
    "        # Hash the record based on join attribute value using hash function H into hash table\n",
    "        s_key = H(s)\n",
    "        if s_key in dic:\n",
    "            dic[s_key].add(s) # If there is an entry\n",
    "        else:\n",
    "            dic[s_key] = {s}\n",
    "    print(dic)\n",
    "    # For each record in table T1 (probing)\n",
    "    for r in T1:\n",
    "        # Hash the record based on join attribute value using H\n",
    "        r_key = H(r)\n",
    "        # If an index entry is found Then\n",
    "        if r_key in dic:\n",
    "            # Compare each record on this index entry with the record of table T1\n",
    "            for item in dic[r_key]:\n",
    "                if item[1] == r[1]:\n",
    "                    # Put the rsult\n",
    "                    result.append( (r[0], r[1], item[0]) )\n",
    "                    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXx6eixAvwPG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: {('Arts', 8)}, 6: {('Business', 15)}, 2: {('Health', 11), ('CompSc', 2)}, 3: {('Dance', 12), ('Finance', 21)}, 7: {('Engineering', 7)}, 1: {('Geology', 10)}, 9: {('IT', 18)}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Adele', 8, 'Arts'), ('Ed', 11, 'Health'), ('Joanna', 2, 'CompSc')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the partitioned result\n",
    "HB_join(R,S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Parallel Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9cpQYKvvH241"
   },
   "outputs": [],
   "source": [
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def consolidate_result(results, result):\n",
    "    # This is called whenever HB_Join(T1, T2) returns a result.\n",
    "    # The results list is modified only by the main process, not the pool workers.\n",
    "    results.append(result)\n",
    "\n",
    "def DPBP_join(T1, T2, n_processor):\n",
    "    \"\"\"\n",
    "    Perform a disjoint partitioning-based parallel join algorithm.\n",
    "    The join attribute is the numeric attribute in the input tables T1 & T2\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "    n_processor -- the number of parallel processors\n",
    "\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    NT1= range_partition(T1,[10,20])\n",
    "    NT2 = range_partition(T2,[10,20])\n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    \n",
    "    callbacks=[]   \n",
    "    results=[]\n",
    "    # Apply local join for each processor\n",
    "    for i in range (len(NT1)):\n",
    "        callbacks.append(pool.apply_async(HB_join, [NT1[i], NT2[i]]))\n",
    "    for callback in callbacks:\n",
    "        for i in callback.get():\n",
    "            results.append(i)\n",
    "    ### END CODE HERE ###\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJgTe8pVH_0z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: {('CompSc', 2)}, 7: {('Engineering', 7)}, 8: {('Arts', 8)}}{1: {('Geology', 10)}, 2: {('Health', 11)}, 3: {('Dance', 12)}, 6: {('Business', 15)}, 9: {('IT', 18)}}{3: {('Finance', 21)}}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Joanna', 2, 'CompSc'), ('Adele', 8, 'Arts'), ('Ed', 11, 'Health')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_processor = 3\n",
    "DPBP_join(R, S, n_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Duplication Outer Join Algorithm (DOJA)\n",
    "\n",
    "In this task, you will implement a **Duplication Outer Join Algorithm (DOJA)**. This algorithm consist of four main steps.\n",
    "1. (Replication): Duplicate the small table \n",
    "2. (Local Inner Join): Perform inner join in each processor\n",
    "3. (Distribution): Reshuffle the inner join result based on R.x\n",
    "4. (Local Outer Join): Perform outer join between the initial table R and the inner join result\n",
    "\n",
    "Assume **n_processor=3** and the dataset is initially partition using the **hash** method. For joining technique, please use the provided **outer join** function for the inner join and outer join operation. **Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R consists of 15 pairs, each comprising two attributes (nominal and numeric)\n",
    "R = [('Adele',8),('Bob',22),('Clement',16),('Dave',23),('Ed',11),\n",
    "     ('Fung',25),('Goel',3),('Harry',17),('Irene',14),('Joanna',2),\n",
    "     ('Kelly',6),('Lim',20),('Meng',1),('Noor',5),('Omar',19)]\n",
    "\n",
    "# S consists of 8 pairs, each comprising two attributes (nominal and numeric)\n",
    "S = [('Arts',8),('Business',15),('CompSc',2),('Dance',12),('Engineering',7),\n",
    "     ('Finance',21),('Geology',10),('Health',11),('IT',18)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def H(r):\n",
    "    \"\"\"\n",
    "    We define a hash function 'H' that is used in the hashing process works \n",
    "    by summing the first and second digits of the hashed attribute, which\n",
    "    in this case is the join attribute. \n",
    "    \n",
    "    Arguments:\n",
    "    r -- a record where hashing will be applied on its join attribute\n",
    "\n",
    "    Return:\n",
    "    result -- the hash index of the record r\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the value of the join attribute into the digits\n",
    "    digits = [int(d) for d in str(r[1])]\n",
    "    \n",
    "    # Calulate the sum of elemenets in the digits\n",
    "    return sum(digits)\n",
    "\n",
    "def outer_join(L, R, join=\"left\"):\n",
    "    \"\"\"outer join using Hash-based join algorithm\"\"\"\n",
    "    \n",
    "    if join == \"right\":\n",
    "        L, R = R, L\n",
    "    \n",
    "    # inner join\n",
    "    if join == \"inner\":\n",
    "        h_dic = {}\n",
    "        for r in R:\n",
    "            h_r = H(r)\n",
    "            if h_r in h_dic.keys():\n",
    "                h_dic[h_r].add(r)\n",
    "            else:\n",
    "                h_dic[h_r] = {r}\n",
    "\n",
    "        result = []\n",
    "        for l in L:\n",
    "            h_l = H(l)\n",
    "            if h_l in h_dic.keys():\n",
    "                for item in h_dic[h_l]:\n",
    "                    if item[1] == l[1]:\n",
    "                        result.append([l[0], item[1], item[0]])\n",
    "        return result\n",
    "    \n",
    "    elif join in [\"left\", \"right\"]:\n",
    "        h_dic = {}\n",
    "        for r in R:\n",
    "#             print(r)\n",
    "            h_r = H(r)\n",
    "            if h_r in h_dic.keys():\n",
    "                h_dic[h_r].add(r)\n",
    "            else:\n",
    "                h_dic[h_r] = {r}\n",
    "#         print(h_dic)\n",
    "                \n",
    "        result = []\n",
    "        for l in L:\n",
    "            isFound = False # to check whether there is a match found.\n",
    "            h_l = H(l)\n",
    "\n",
    "            if h_l in h_dic.keys():\n",
    "                for item in h_dic[h_l]:\n",
    "                    if item[1] == l[1]:\n",
    "                        result.append([l[0], item[1], item[2]])\n",
    "                        isFound = True\n",
    "                        break\n",
    "                        \n",
    "            if not isFound:\n",
    "                result.append([l[0], l[1],str(np.nan)])\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        raise AttributeError('join should be in {left, right, inner}.') \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_distribution(T, n):\n",
    "    \"\"\"distribute data using hash partitioning\"\"\"\n",
    "    \n",
    "    # Define a simple hash function for demonstration\n",
    "    def s_hash(x, n):\n",
    "        h = x%n \n",
    "        return h\n",
    "    \n",
    "    result = {p:[] for p in range(n)}\n",
    "    for t in T:\n",
    "        h_key = s_hash(t[1], n)\n",
    "        result[h_key].append(tuple(t))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def DOJA(L, R, n):\n",
    "    \"\"\"left outer join using DOJA\"\"\"\n",
    "    \n",
    "    ### Start CODE\n",
    "    \n",
    "    pool = mp.Pool(processes =n)\n",
    "    l_dis = hash_distribution(L,n)\n",
    "    r_dis = hash_distribution(R,n)\n",
    "    first_Call=[]\n",
    "#     print(L)\n",
    "#     print(r_dis)\n",
    "    first_Call=[]\n",
    "    if len(L)<=len(R):\n",
    "        Dup_lst=L\n",
    "    else:\n",
    "        Dup_lst = R\n",
    "    for i in range(n):\n",
    "        first_Call.append(pool.apply_async(outer_join,[Dup_lst,r_dis[i],\"inner\"]))\n",
    "    \n",
    "    output=[]\n",
    "    for x in first_Call:\n",
    "        for i in x.get():\n",
    "            \n",
    "            output.append(i)\n",
    "    o_dis = hash_distribution(output,n)\n",
    "#     print(o_dis)\n",
    "    callbacks=[]\n",
    "    for r in range(n):\n",
    "        callbacks.append(pool.apply_async(outer_join,[l_dis[r],o_dis[r],\"left\"]))\n",
    "        \n",
    "    \n",
    "    output=[]\n",
    "    for x in callbacks:\n",
    "            for i in x.get():\n",
    "                output.append(i)\n",
    "#     ### End CODE\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Goel', 3, 'nan'],\n",
       " ['Kelly', 6, 'nan'],\n",
       " ['Bob', 22, 'nan'],\n",
       " ['Clement', 16, 'nan'],\n",
       " ['Fung', 25, 'nan'],\n",
       " ['Meng', 1, 'nan'],\n",
       " ['Omar', 19, 'nan'],\n",
       " ['Adele', 8, 'Arts'],\n",
       " ['Dave', 23, 'nan'],\n",
       " ['Ed', 11, 'Health'],\n",
       " ['Harry', 17, 'nan'],\n",
       " ['Irene', 14, 'nan'],\n",
       " ['Joanna', 2, 'CompSc'],\n",
       " ['Lim', 20, 'nan'],\n",
       " ['Noor', 5, 'nan']]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOJA(R,S,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Parallel Join Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly answer the following question.\n",
    "\n",
    "1. For each partitioning algorithm besides range-partitioning, state and justify whether we can use it with the code for disjoint-partitioning based parallel join above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**:\n",
    "For hash partition we are able to do so since the code above is using it and we are ensure the position of each data located in which partition so that when we parallel join will not missed any of the data.\n",
    "\n",
    "But for round-robin partition it will not able to ensure the position of the data in each partition so there might be chance where the results will missed some data that could be joined.\n",
    "\n",
    "Other than this two the random-unequal data partition also faced the same problem as round-robin partition sinced it cannot ensure each position of the data and there might be chanced it missed out an data that could be joined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9vIqdbjJaXv"
   },
   "source": [
    "## 3. Parallel Sorting Algorithm (2 marks)\n",
    "\n",
    "In this task, you will implement **parallel binary-merge sort** method. It has two phases same as the parallel merge-all sort that you learnt in the labs.\n",
    "1. Local sort\n",
    "2. Final merge.\n",
    "\n",
    "The first phase is similar to the parallel merge-all sort. The second phase, the merging phase, is pipelined instead of concentrating on one processor. In this phase, we take the results from two processors and then merge the two in one processor, called binary merging. The result of the merge between two processors is passed on to the next level until one processor (the host) is left.\n",
    "\n",
    " **Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**\n",
    "Assume that we use the round robin partitioning method  (i.e. `rr_partition()`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Round-robin Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m5p8cRn73Zqc"
   },
   "outputs": [],
   "source": [
    "# Round-robin data partitionining function\n",
    "def rr_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    result = [[] for i in range (n)]\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    for index, element in enumerate(data): \n",
    "\n",
    "        index_bin = (int) (index % n)\n",
    "        result[index_bin].append(element)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jk-Ch1Jm3iCC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Adele', 8), ('Dave', 23), ('Goel', 3), ('Joanna', 2), ('Meng', 1)],\n",
      " [('Bob', 22), ('Ed', 11), ('Harry', 17), ('Kelly', 6), ('Noor', 5)],\n",
      " [('Clement', 16), ('Fung', 25), ('Irene', 14), ('Lim', 20), ('Omar', 19)]]\n"
     ]
    }
   ],
   "source": [
    "# Test the round-robin partitioning function\n",
    "result = rr_partition(R, 3)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Serial Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gd16AZF_LgWp"
   },
   "outputs": [],
   "source": [
    "def qsort(arr): \n",
    "    \"\"\" \n",
    "    Quicksort a list\n",
    "    \n",
    "    Arguments:\n",
    "    arr -- the input list to be sorted\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted arr\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    else:\n",
    "        # take the first element as the pivot\n",
    "        pivot = arr[0]\n",
    "        left_arr = [x for x in arr[1:] if x[1] < pivot[1]]\n",
    "        right_arr = [x for x in arr[1:] if x[1] >= pivot[1]]\n",
    "        # uncomment this to see what to print \n",
    "        # print(\"Left:\" + str(left_arr)+\" Pivot : \"+ str(pivot)+\" Right: \" + str(right_arr))\n",
    "        sorted_arr = qsort(left_arr) + [pivot] + qsort(right_arr)\n",
    "        \n",
    "        return sorted_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8kIDuxV1uGS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Meng', 1),\n",
       " ('Joanna', 2),\n",
       " ('Goel', 3),\n",
       " ('Noor', 5),\n",
       " ('Kelly', 6),\n",
       " ('Adele', 8),\n",
       " ('Ed', 11),\n",
       " ('Irene', 14),\n",
       " ('Clement', 16),\n",
       " ('Harry', 17),\n",
       " ('Omar', 19),\n",
       " ('Lim', 20),\n",
       " ('Bob', 22),\n",
       " ('Dave', 23),\n",
       " ('Fung', 25)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qsort(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3vxcrs-LVaG"
   },
   "outputs": [],
   "source": [
    "# Let's first look at 'k-way merging algorithm' that will be used \n",
    "# to merge sub-record sets in our external sorting algorithm.\n",
    "import sys\n",
    "\n",
    "# Find the smallest record\n",
    "def find_min(records):    \n",
    "    \"\"\" \n",
    "    Find the smallest record\n",
    "    \n",
    "    Arguments:\n",
    "    records -- the input record set\n",
    "\n",
    "    Return:\n",
    "    result -- the smallest record's index\n",
    "    \"\"\"\n",
    "    m = records[0]\n",
    "    index = 0\n",
    "    for i in range(len(records)):\n",
    "        if(records[i][1] < m[1]):  \n",
    "            index = i\n",
    "            m = records[i]\n",
    "    return index\n",
    "\n",
    "def k_way_merge(record_sets):\n",
    "    \"\"\" \n",
    "    K-way merging algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    record_sets -- the set of multiple sorted sub-record sets\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted and merged record set\n",
    "    \"\"\"\n",
    "    # indexes will keep the indexes of sorted records in the given buffers\n",
    "    indexes = []\n",
    "    for _ in record_sets:\n",
    "        indexes.append(0) # initialisation with 0\n",
    "\n",
    "    # final result will be stored in this variable\n",
    "    result = []\n",
    "    while(True):\n",
    "        merged_result = [] # the merging unit (i.e. # of the given buffers)\n",
    "        \n",
    "        # This loop gets the current position of every buffer\n",
    "        for i in range(len(record_sets)):\n",
    "            if(indexes[i] >= len(record_sets[i])):\n",
    "                merged_result.append((None, sys.maxsize))\n",
    "            else:\n",
    "                merged_result.append(record_sets[i][indexes[i]])  \n",
    "        \n",
    "        # find the smallest record \n",
    "        smallest = find_min(merged_result)\n",
    "        \n",
    "        # if we only have sys.maxsize on the tuple, we reached the end of every record set\n",
    "        if(merged_result[smallest][1] == sys.maxsize):\n",
    "            break\n",
    "\n",
    "        # This record is the next on the merged list\n",
    "        result.append(record_sets[smallest][indexes[smallest]])\n",
    "        indexes[smallest] +=1\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMrHqU8o3Mh7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Adele', 8), ('Dave', 23), ('Goel', 3), ('Joanna', 2), ('Meng', 1)], [('Bob', 22), ('Ed', 11), ('Harry', 17), ('Kelly', 6), ('Noor', 5)], [('Clement', 16), ('Fung', 25), ('Irene', 14), ('Lim', 20), ('Omar', 19)]]\n",
      "[('Meng', 1),\n",
      " ('Joanna', 2),\n",
      " ('Goel', 3),\n",
      " ('Noor', 5),\n",
      " ('Kelly', 6),\n",
      " ('Adele', 8),\n",
      " ('Ed', 11),\n",
      " ('Irene', 14),\n",
      " ('Clement', 16),\n",
      " ('Harry', 17),\n",
      " ('Omar', 19),\n",
      " ('Lim', 20),\n",
      " ('Bob', 22),\n",
      " ('Dave', 23),\n",
      " ('Fung', 25)]\n"
     ]
    }
   ],
   "source": [
    "# Test k-way merging method\n",
    "buffers = rr_partition(R, 3)\n",
    "print(buffers)\n",
    "result = k_way_merge([qsort(b) for b in buffers])\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yof8Q84YLcOU"
   },
   "outputs": [],
   "source": [
    "def serial_sorting(dataset, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a serial external sorting method based on sort-merge\n",
    "    The buffer size determines the size of each sub-record set\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- the entire record set to be sorted\n",
    "    buffer_size -- the buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted record set\n",
    "    \"\"\"\n",
    "    \n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "    ### START CODE HERE ### \n",
    "    sorted_set = []\n",
    "    start_pos =0\n",
    "    N= len(dataset)\n",
    "    \n",
    "    while True :\n",
    "        if start_pos +buffer_size <N:\n",
    "            subset = dataset[start_pos: start_pos +buffer_size]\n",
    "            sorted_set.append(qsort(subset))\n",
    "            start_pos +=buffer_size\n",
    "            \n",
    "        else:\n",
    "            subset = dataset[start_pos:]\n",
    "            sorted_set.append(qsort(subset))\n",
    "            break\n",
    "#     print(sorted_set)\n",
    "    dataset = sorted_set\n",
    "    merge_buffer_size = buffer_size -1\n",
    "    while True:\n",
    "        start_pos =0\n",
    "        merged_set = []\n",
    "        N=len(dataset)\n",
    "        \n",
    "        while True:\n",
    "            if start_pos +buffer_size <N:\n",
    "                subset = dataset[start_pos: start_pos +merge_buffer_size]\n",
    "                sorted_set.append(k_way_merge(subset))\n",
    "                start_pos +=merge_buffer_size\n",
    "            \n",
    "            else:\n",
    "                subset = dataset[start_pos:]\n",
    "                merged_set.append(k_way_merge(subset))\n",
    "                break\n",
    "    \n",
    "        dataset =  merged_set\n",
    "        if len(dataset)==1:\n",
    "            break\n",
    "    result=dataset\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Adele', 8), ('Bob', 22), ('Clement', 16), ('Dave', 23), ('Ed', 11), ('Fung', 25), ('Goel', 3), ('Harry', 17), ('Irene', 14), ('Joanna', 2), ('Kelly', 6), ('Lim', 20), ('Meng', 1), ('Noor', 5), ('Omar', 19)]\n"
     ]
    }
   ],
   "source": [
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmW47ALF6bH0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final sorting result:[[('Meng', 1), ('Joanna', 2), ('Goel', 3), ('Noor', 5), ('Kelly', 6), ('Adele', 8), ('Ed', 11), ('Irene', 14), ('Clement', 16), ('Harry', 17), ('Omar', 19), ('Lim', 20), ('Bob', 22), ('Dave', 23), ('Fung', 25)]]\n"
     ]
    }
   ],
   "source": [
    "result = serial_sorting(R, 4)\n",
    "print(\"final sorting result:\" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Parallel Binary-merge Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_jH8jXwLKRT"
   },
   "outputs": [],
   "source": [
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_binary_merge_sorting(dataset, n_processor, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a parallel binary-merge sorting method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be sorted\n",
    "    n_processor -- number of parallel processors\n",
    "    buffer_size -- buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the merged record set\n",
    "    \"\"\"\n",
    "    \n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    subsets = rr_partition(dataset,n_processor)\n",
    "    \n",
    "    pool = mp.Pool(processes= n_processor)\n",
    "    \n",
    "    sorted_set = []\n",
    "    callbacks = []\n",
    "    for s in subsets:\n",
    "        callbacks.append(pool.apply_async(serial_sorting,[s,buffer_size]))\n",
    "    \n",
    "    for c in callbacks:\n",
    "        sorted_set += c.get()\n",
    "        \n",
    "    \n",
    "    while len(sorted_set)!=1:\n",
    "        \n",
    "        temp_lst=[]\n",
    "        callbacks_m=[]\n",
    "#         print(len(sorted_set))\n",
    "        for i in range (len(sorted_set)//2):\n",
    "             callbacks_m.append(pool.apply_async(k_way_merge,[[sorted_set[i*2],sorted_set[i*2+1]]]))\n",
    "        temp_lst=[r.get() for r in callbacks_m]\n",
    "        if (i*2+1)!= len(sorted_set)-1:\n",
    "            temp_lst.append(sorted_set[-1])\n",
    "#         print(temp_lst)\n",
    "        sorted_set=temp_lst\n",
    "    result = sorted_set\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aMu19MwXLxNd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:[[('Meng', 1), ('Joanna', 2), ('Goel', 3), ('Noor', 5), ('Kelly', 6), ('Adele', 8), ('Ed', 11), ('Irene', 14), ('Clement', 16), ('Harry', 17), ('Omar', 19), ('Lim', 20), ('Bob', 22), ('Dave', 23), ('Fung', 25)]]\n"
     ]
    }
   ],
   "source": [
    "result = parallel_binary_merge_sorting(R, 10, 20)\n",
    "print(\"final result:\" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Parallel Binary-merge Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly answer the following.\n",
    "\n",
    "1. Does parallel binary-merge utilize all available processors? State and justify with respect to each phase: sort, merge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**:\n",
    "For the sort phase we utilize all availble processors since we partition by using the number of processors but for the merging state we using binary merge so for the first merging step we only use 5 of the processors since 10/2 = 5 and each time we merge we will decrease the usage of processors by halved so we aren't using all available processors for merging state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Parallel GroupBy (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Parallel GroupBy with Merge-All\n",
    "\n",
    "In this task, you will implement **Parallel GroupBy with Merge-All** method. It invloves the following two steps:\n",
    "1. Local aggregate in each processor\n",
    "2. Global aggregation\n",
    "\n",
    "Assume **n_processor=4** and the dataset has already been pre-partitioned. For local aggregation, please use the provided **local_groupby** function. **Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = [('A', 1), ('B', 2), ('C', 3), ('A', 10), ('B', 20), ('C', 30)]\n",
    "D2 = [('A', 2), ('B', 3), ('C', 4), ('A', 20), ('B', 30), ('C', 40)]\n",
    "D3 = [('A', 3), ('B', 4), ('C', 5), ('A', 30), ('B', 40), ('C', 50)]\n",
    "D4 = [('A', 4), ('B', 5), ('C', 6), ('A', 40), ('B', 50), ('C', 60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step in the merge-all groupby method\n",
    "def local_groupby(dataset):\n",
    "    \"\"\"\n",
    "    Perform a local groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record set according to the group_by attribute index\n",
    "    \"\"\"\n",
    "\n",
    "    dict = {}\n",
    "    for index, record in enumerate(dataset):\n",
    "        key = record[0]\n",
    "        val = record[1]\n",
    "        if key not in dict:\n",
    "            dict[key] = 0\n",
    "        dict[key] += val\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 11, 'B': 22, 'C': 33}\n"
     ]
    }
   ],
   "source": [
    "result = local_groupby (D1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_merge_all_groupby(dataset):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge_all groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index\n",
    "    \"\"\"\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    n_processor = len(dataset)\n",
    "    \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    \n",
    "    local_result = []\n",
    "    callbacks=[]\n",
    "    for s in dataset:\n",
    "        callbacks.append(pool.apply_async(local_groupby, [s]))\n",
    "    local_result=[r.get() for r in callbacks]\n",
    "    pool.close()\n",
    "#     print(local_result)\n",
    "    \n",
    "    \n",
    "    for r in local_result:\n",
    "        for key in r.keys():\n",
    "            if key not in result:\n",
    "                result[key]=0\n",
    "            result[key] +=r[key]\n",
    "       \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 110, 'B': 154, 'C': 198}\n"
     ]
    }
   ],
   "source": [
    "E = [D1, D2, D3, D4]\n",
    "result = parallel_merge_all_groupby (E)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Redistribution Method\n",
    "\n",
    "In this task, you will implement **Parallel GroupBy with Redistribution Method** method. It invloves the following two steps:\n",
    "1. (Partitioning phase): Redistribute raw records to all processor\n",
    "2. (Aggregation phase): Each processor performs a local aggregation\n",
    "\n",
    "\n",
    "Assume **n_processor=4** and the dataset should be partitioned into 4 groups using **range partition** Where first group with A & B, second with C & D, third with E & F and fourth with G & H. For local aggregation, please use the provided **local_groupby** function. **Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = [('A', 1), ('B', 2), ('C', 3), ('D', 10), ('E', 20), ('F', 30), ('G', 20), ('H', 30), \n",
    "      ('A', 2), ('B', 3), ('C', 4), ('D', 20), ('E', 30), ('F', 40), ('G', 30), ('H', 40),\n",
    "      ('A', 3), ('B', 4), ('C', 5), ('D', 30), ('E', 40), ('F', 50), ('G', 40), ('H', 50),\n",
    "      ('A', 4), ('B', 5), ('C', 6), ('D', 40), ('E', 50), ('F', 60), ('G', 50), ('H', 60)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create range partition function based on GroupBy attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range data partitionining function (Need to modify as instructed above)\n",
    "def range_partition(data, range_indices):\n",
    "    \"\"\"\n",
    "    Perform range data partitioning on data based on the join attribute\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset (list of tuple)\n",
    "    range_indices -- the range end-points (e.g., ['C', 'E', 'G'] means\n",
    "                        4 intervals: x < 'C', 'C' <= x < 'E', 'E' <= x < 'G', 'G' <= x)\n",
    "\n",
    "    Return:\n",
    "    result -- the partitioned subsets of D\n",
    "    \"\"\"\n",
    "    result = [[] for i in range (len(range_indices)+1)]\n",
    "    \n",
    "    \n",
    "    ### START CODE HERE ###  \n",
    "    for i in D:\n",
    "        flag=False\n",
    "        for j in range (len(range_indices)):\n",
    "            if i[0]<range_indices[j]:\n",
    "                result[j].append(i)\n",
    "                flag = True\n",
    "    \n",
    "                break\n",
    "        if flag != True:\n",
    "            result[j+1].append(i)\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_redistributed_groupby(dataset, range_indices):\n",
    "    \"\"\"\n",
    "    Perform a parallel redistributed groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    distributed_set = range_partition(dataset,range_indices)\n",
    "#     print(distributed_set)\n",
    "    pool = mp.Pool(processes= len(range_indices)+1)\n",
    "    local_result=[]\n",
    "    callbacks=[]\n",
    "    for i in distributed_set:\n",
    "        callbacks.append(pool.apply_async(local_groupby,[i]))\n",
    "    local_result=[r.get() for r in callbacks]\n",
    "    pool.close()\n",
    "#     print(local_result)\n",
    "    merge_dic={}\n",
    "    for i in local_result:\n",
    "        merge_dic.update(i)\n",
    "    result.append(merge_dic)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'A': 10, 'B': 14, 'C': 18, 'D': 100, 'E': 140, 'F': 180, 'G': 140, 'H': 180}]\n"
     ]
    }
   ],
   "source": [
    "result = parallel_redistributed_groupby (D, ['C', 'E', 'G'])\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT3182 - Take_Home_Test_Solution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
